{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOyO0iGSEoIu"
      },
      "outputs": [],
      "source": [
        "### What you have:\n",
        "# 1. A dataset of movie reviews from imdb structured as\n",
        "# \"review text\", \"good\"\n",
        "# \"review text\", \"bad\"\n",
        "# \"review text\", \"bad\",\n",
        "# ...\n",
        "#\"review text\", \"good\"\n",
        "#\n",
        "# the second column indicates of the review was positive or negative\n",
        "# the data is stored in the file \"movie_reviews.csv\"\n",
        "#\n",
        "# 2. The GPT2 transformer model\n",
        "\n",
        "### Task 1: Finetune the model to complete movie review text. Examples:\n",
        "#           \"The movie was fantastic because\",\n",
        "#           \"I didn't like the film because\",\n",
        "#           \"One of the best performances was\"\n",
        "#\n",
        "# Use the template below\n",
        "# you may need to take care of additional library import/installation depending on your system\n",
        "\n",
        "### Task 2: The second column in the \"movie_reviews.csv\" file indicates if the review was positive or negative. Without adding any more layers to GPT2, finetune the model to tell you wheather a review is positive or negative.\n",
        "\n",
        "### The jupyter notebook with the template and the dataset movie_reviews are included with this email.\n",
        "\n",
        "### Note: you can always downsample the dataset if the training is taking an accessively long time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc2nUkfMJ0WM"
      },
      "outputs": [],
      "source": [
        "# may be needed if you are running this in a google colab\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IXjWNDgPJ5Hj"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForSequenceClassification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "                                     TASK - 1 - TEXT GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ujWwuwmpKOs2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Load fine tuned GPT-2 Model and Tokenizer for generation\n",
        "tokenizer_gen = GPT2Tokenizer.from_pretrained('/Users/aravindryali/Desktop/Studies/AITask/outputs_gen/fine_tuned_gpt2')\n",
        "model_gen = GPT2LMHeadModel.from_pretrained('/Users/aravindryali/Desktop/Studies/AITask/outputs_gen/fine_tuned_gpt2')\n",
        "tokenizer_gen.pad_token = tokenizer_gen.eos_token  # Set pad_token to eos_token\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_gen.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g6JVgnhiKWCa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: The movie was fantastic because\n",
            "Generated Text: The movie was fantastic because of the acting. The story line is very realistic and believable, with some pretty funny moments such as when a woman (and her cat) try to escape from an abusive husband by calling him \"Mr.\" He does not seem too evil or dangerous at times but doesn't appear much like his brother-in law in this film.There are also several interesting character relationships between these characters that I really enjoyed watching - especially John Belushi's girlfriend/friend\n",
            "\n",
            "\n",
            "Prompt: I didn't like the film because\n",
            "Generated Text: I didn't like the film because I was not expecting much. The only reason it's a good movie is that there are some scenes of nudity and violence which makes things even more interesting.The acting in this one really works for me, especially considering what they did with Michael Caine as his partner-in law to get him killed off (as he already knew how). There isn\n",
            "\n",
            "\n",
            "\n",
            "Prompt: One of the best performances was\n",
            "Generated Text: One of the best performances was by Paul Reiser, who does a good job as he plays his character. The story is told in such an interesting way that you think \"why did I watch this?\" But it turns out to be really funny.The acting and writing are excellent - there's something about these characters they're not caricatures but real people with genuine feelings for their victims (there were some decent lines from both O'Leary & Colburn). There isn't\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Run inference (on a few examples):\n",
        "\n",
        "# Task 1:\n",
        "import torch, re\n",
        "\n",
        "def clean_generated_text(text):\n",
        "    # Remove HTML-like tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def generate_text(prompt, model, tokenizer, max_length=100, device='cpu'):\n",
        "    # Ensure the model and inputs are on the same device\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Set attention mask\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Generate text using the model\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,# Ensure padding is handled correctly\n",
        "            no_repeat_ngram_size=2,               # Optional: prevent repeating n-grams\n",
        "            repetition_penalty=2.0,               # Optional: avoid repetitive phrases\n",
        "            top_k=50,                             # Optional: limit sampling to top-k tokens\n",
        "            top_p=0.95,                           # Optional: nucleus sampling\n",
        "            temperature=0.7,                      # Optional: control randomness\n",
        "            do_sample=True                        # Optional: enable sampling\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens back into text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    clean_text = clean_generated_text(generated_text)\n",
        "    return clean_text\n",
        "\n",
        "# Example prompts\n",
        "prompts = [\n",
        "    \"The movie was fantastic because\",\n",
        "    \"I didn't like the film because\",\n",
        "    \"One of the best performances was\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {generate_text(prompt, model_gen, tokenizer_gen, device=device)}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "                                    TASK - 2 - REVIEW CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load fine tuned GPT-2 Model and Tokenizer for review classification\n",
        "tokenizer_cls = GPT2Tokenizer.from_pretrained('/Users/aravindryali/Desktop/Studies/AITask/cls_output/gpt2-movie-reviews-classifier')\n",
        "model_cls = GPT2ForSequenceClassification.from_pretrained('/Users/aravindryali/Desktop/Studies/AITask/cls_output/gpt2-movie-reviews-classifier')\n",
        "tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_gen.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: The movie was fantastic and the screenplay is amazing\n",
            "Prediction: Positive\n",
            "\n",
            "Review: I didn't enjoy the film. It was very boring\n",
            "Prediction: Positive\n",
            "\n",
            "Review: One of the best experiences ever. worth watching.\n",
            "Prediction: Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict review if its is good or bad\n",
        "def predict_review(review, model_cls, tokenizer_cls):\n",
        "    inputs = tokenizer_cls(review, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model_cls(**inputs)\n",
        "        \n",
        "    logits = outputs.logits\n",
        "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
        "    \n",
        "    if predicted_class_id == 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Positive\"\n",
        "    \n",
        "# Example reviews for predicting\n",
        "reviews = [\n",
        "    \"The movie was fantastic and the screenplay is amazing\",\n",
        "    \"I didn't enjoy the film. It was very boring\",\n",
        "    \"One of the best experiences ever. worth watching.\"\n",
        "]\n",
        "\n",
        "# Predict and print the results\n",
        "for review in reviews:\n",
        "    prediction = predict_review(review, model_cls, tokenizer_cls)\n",
        "    print(f\"Review: {review}\\nPrediction: {prediction}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
